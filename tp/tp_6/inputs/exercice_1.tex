\chapter{RNN simple pour Arduino}

\section{But de l'exercice}
Le but de l'exercice est de simuler un réseau de neurones récurrents (\textbf{RNN}) pour \textbf{prédire} une séquence de sortie en fonction d'une séquence d'entrée. 
Le \textbf{RNN} est composé de trois couches de neurones: une couche d'entrée, une couche cachée et une
couche de sortie. Les poids et les biais de chaque couche sont initialisés aléatoirement à l'aide
de la fonction d'initialisation des poids. Les sorties sont calculées par l'algorithme de
\textbf{``propagation avant''} de l'entrée à travers le réseau pour calculer la sortie en utilisant la fonction
d'activation \textbf{sigmoid}. Le seuil de prédiction de sortie est donné par la valeur lue d'une
fonction \textbf{valpot} et ajuste le seuil de prédiction en conséquence. La prédiction est initialisé par
la séquence d'entrée avec des valeurs aléatoires et démarre la séquence de prédiction. Dans
la boucle \textbf{loop()}, la séquence d'entrée est avancée à chaque itération et la séquence de sortie
est prédite. La valeur prédite est affichée sur la \textbf{LED interne} de l'arduino Due en fonction du
seuil de prédiction. \\

Le code de cet exercice est fourni en annexe. \hyperref[sec:RNN-code]{Code de l'exercice}.
\section{Réponse à l'exercice}
\begin{enumerate}
  \item {
    \textbf{Quel est le rôle de la fonction initializeWeights?} \vspace{0.2cm}\\
    Le rôle de la fonction \textbf{initializeWeights} est d'intialiser les valeurs des \textbf{poids} et des \textbf{biais} contenus 
    dans les différentes matrices \textbf{weightsIH}, \textbf{weightsHH}, \textbf{weightsHO}, \textbf{biasH} et \textbf{biasO}. 
    Les différentes valeurs sont initialisées aléatoirement entre \textbf{0} et \textbf{1}.
  } \\
  \item {
    \textbf{Comment est calculé l'état caché (hidden state) dans la fonction forwardPass?} \vspace{0.2cm}\\
    La fonction \textbf{forwardPass} calcule l'\textbf{état caché} en effectuant une série d'opérations de calculs sur les entrées fournies
    dans le tableau d'entrées ainsi que sur les différents biais et poids du réseau. \\
    Plus précisément, la fonction calcule l'\textbf{état caché} en effectuant les opérations suivantes:
    \begin{enumerate}
      \item \textbf{Initialiser} les valeurs de l'état caché à \textbf{0}.
      \item Pour chaque neurone d'entrée, \textbf{multiplier }la valeur de l'entrée correspondante par le poids correspondant entre l'entrée et le neurone caché.
      \item Pour chaque neurone caché, \textbf{multiplier} la valeur de son état actuel par le poids correspondant entre ce neurone et le neurone caché considéré.
      \item \textbf{Ajouter} la somme de ces produits pondérés à la valeur du biais du neurone caché considéré.
      \item \textbf{Appliquer} une fonction d'activation à la somme pondérée obtenue pour obtenir la valeur finale de l'état caché pour le neurone considéré.
    \end{enumerate}
    Ces étapes sont effectuées pour \textbf{chaque neurone caché} dans le réseau, ce qui permet de calculer l'état caché complet du réseau de neurones.
  } \\
  \item {
    \textbf{Quel est l'avantage de l'utilisation de la fonction d'activation sigmoid pour calculer les sorties du réseau de neurones?}\vspace{0.2cm}\\
    Les avantages de l'utilisation de la fonction \textbf{sigmoïd} en tant que fonction d'activation sont qu'elle possède une plage de sortie \textbf{entre 0 et 1}, ce 
    qui est particulièrement adapté pour des problèmes de \textbf{classification binaire}, notre cas d'étude nécéssite une \textbf{sortie binaire 0 ou 1} pour l'état de la LED.\@
  } \\
  \item {
    \textbf{A quoi sert la fonction readPoten et comment est-elle utilisée pour ajuster le seuil de prédiction?} \vspace{0.2cm}\\
    La fonction \textbf{readPoten} permet de lire la valeur du potentiomètre. Par la suite elle \textbf{map} cette valeur entre 0 et 1 et fixe le treshold sur cette dernière.\\
    Le treshold permet de \textbf{considérer} si une sortie est une valeur positive ou négative. Dans notre cas positive représente allumer la LED et négative représente éteindre la LED.\@
  } \\
  \item {
    \textbf{A quoi sert la fonction startPrediction et comment est-elle utilisée pour calculer la prédiction?} \vspace{0.2cm}\\
    La fonction \textbf{startPrediction} initialise les valeurs du tableau d'entrée entre 0 et 1. Par la suite, elle met la valeur de la variable \textbf{predict} à la valeur \textbf{true}.
    Cette valeur de la variable \textbf{predict} permet d'indiquer à la boucle \textbf{loop} du programme de commencer la prédiction du réseau sur le tableau d'entrée.\\
    En résumé cette fonction possède à la fois un rôle d'\textbf{initialisation} mais également d'\textbf{initiatrice} du réseau de neurone.
  } \\
  \item {
    \textbf{Expliquer comment le RNN calcule la sortie avec la fonction forwardPass et comment on peut faire pour étendre le nombre des cellules RNN?} \vspace{0.2cm}\\
    Le \textbf{RNN} utilise la fonction \textbf{forwardPass} pour calculer la \textbf{sortie} du réseau en fonction de \textbf{l'état caché} et des \textbf{entrées}. Cette fonction 
    prend en entrée un \textbf{tableau d'entrée}, un \textbf{tableau de sortie} et un \textbf{tableau d'état caché}. La fonction calcule l'\textbf{état caché} en utilisant les \textbf{entrées} et l'état \textbf{caché}
    de la couche précédente, en utilisant les \textbf{poids} et les \textbf{biais}. Ensuite, elle passe cette somme pondérée à une \textbf{fonction d'activation} pour obtenir l'état caché de la cellule. Ceci est \textbf{répété} pour chaque 
    cellule de la couche cachée. \\
    Ensuite la fonction \textbf{forwardPass} calcule la séquence de sortie en bouclant sur les \textbf{cellules de sortie} du \textbf{RNN}. Pour chaque cellule de sortie, la fonction calcule la \textbf{somme pondérée} des états
    cachés de toutes les cellules RNN en utilisant les \textbf{poids weightsHO} et le biais \textbf{biasO}. Cette somme pondérée est ensuite passée à la fonction d'activation pour \textbf{obtenir la sortie} de la cellule. \\
    
    Pour étendre le nombre de \textbf{cellules RNN} dans le réseau, il faut ajouter des cellules RNN supplémentaires à la couche cachée. Cela implique d'augmenter la taille des tableaux \textbf{hidden}, \textbf{weightsHH}.
    Il faut également modifier la fonction \textbf{forwardPass} pour prendre en compte ces nouvelles cellules.
  }
\end{enumerate}